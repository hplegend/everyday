

## CNN 基础

[TOC]

#### sigmod 函数

![img](../img/cnn/SIGMOD)

what is sigmod函数？ 在机器学习中，sigmod被广泛的应用在非线性变换中，实际上是用于预测结果趋近于正例还是反例，换句话说就是根据sigmod变换的结果，判断是接近0还是接近1（如果我们用0表示反例，1表示正例）。 从上面的sigmod的函数曲线可以看成，当X非常大时，函数值趋近1；当X非常小时，函数值趋近0。sigmod函数的函数值在[0~1]之间，跟我们的通常说的概率在一个区间，也就是说输入值X在(-∞，+∞)，经过sigmod函数后，都会被映射到[0~1]之间。对于概率，我们都知道表示的一种接近程度，在这里sigmod函数干的事情，就是把输入X转换成一种分类概率，更接近1表示正样本，更接近0表示负样本。

#### 神经元与神经网络



![img](../img/cnn/nerualpic)



神经元，神经网络的基本组成单元。回想一下生物课上学习的神经元，是不是有输入和输出，这里的神经元跟生物上的神经元类似，也有输入和输出。多个神经元按照一定的层次结构便可以形成神经网络。

![img](../img/cnn/nerualnetwork)

上面的图就是一个简单的三次神经网络，由多个神经元组成层次结构。信息输入到神经网络结构中，经过线性或者非线性变换被存储在网络中，最后给出输出。上面的三层网络中的每一层，我们都有一个名称： 输入层，隐层，输出层（从左到右边）。每一层的输出都是下一层的输入，例如：输入层的输出是隐层的输入，隐层的输出是输出层的输出。

![img](../img/cnn/nerualnetworkwithbias)

实际上，神经网络的输入层和隐层都会存在一个偏置（$y=ax+b$），其实就是允许的偏差。根据上面的图，我们可以得到如下的公式：

![img](../img/cnn/nerualfoumular)

g(X)中的X就是输入，因为输入层的每一个输出都是隐层的输入，因此可以很好的理解上面的公式。



#### 卷积神经网络

前面我们已经了解到，神经网络由多个神经元组成层次结构，最近的网络结构：输入层，隐层，输出层。我们可以再这个基础的结构上继续扩展，比如说加更多的隐层。

![img](../img/cnn/multiimplicitlayer)

如果隐层的层数加的足够多，我们就会形成如下的网络结构：

![img](../img/cnn/CNN)

上图就是我们常说的卷积神经网络（CNN），CONV -- 卷积层，RULE-- 激励层，POOL--池化层，FC-- 全连接层。 CNN搞这么多层，到底是要解决什么问题呢？我们都知道神经网络具有知识存储的能力，CNN要做的事情就是训练网络，让网络记住某一个特征的物体，当下次给出一个物体时，CNN能快速的辨别物体是不是我们想要的。 就拿上面的图来说吧，CNN要做的事情，就是给定一副图片，判断一下这幅图中有没有车（其实很有更进一步的：如果有车，是什么车）。



#### reference

通俗理解卷积神经网络   https://blog.csdn.net/v_july_v/article/details/51812459

